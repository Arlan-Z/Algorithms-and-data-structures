# Complexity and Memory
![Markdown](https://www.bigocheatsheet.com/img/big-o-cheat-sheet-poster.png)

## Временная сложность
Временная сложность алгоритма определяет количество
времени, затрачиваемого алгоритмом на выполнение, как функцию от длины
входных данных. Обратите внимание, что время выполнения является функцией
длины входных данных, а не фактического времени выполнения машины, на
которой работает алгоритм.
***
Действительный алгоритм требует конечного времени для выполнения. Время,
необходимое алгоритму для решения поставленной задачи, называется
временной сложностью алгоритма. Временная сложность является очень
полезной мерой при анализе алгоритмов.
Она представляет собой время, необходимое для завершения работы алгоритма.
Для оценки временной сложности необходимо учесть стоимость каждой
фундаментальной инструкции и количество ее выполнения.

#### Пример 1: Сложение двух скалярных переменных

 > Алгоритм ADD SCALAR(A, B)

 
 ```
    Описание: Выполнить арифметическое сложение двух чисел
    Вход: две скалярные переменные A и B
    Выход: переменная C, содержащая результат сложения A и B
    C <- A + B
    возврат C
 ```

Для сложения двух скалярных чисел требуется одна операция сложения.
Временная сложность этого алгоритма постоянна, поэтому T(n) = O(1) .
Для расчета временной сложности алгоритма предполагается, что на выполнение
одной операции затрачивается постоянное время c, а затем вычисляется общее
количество операций для входа длиной на N. Для понимания процесса
вычислений рассмотрим пример: Предположим, задача состоит в том, чтобы
найти, существует ли пара (X, Y) в массиве A из N элементов, сумма которых равна Z. Простейшая идея - рассматривать каждую пару и проверять, удовлетворяет она
заданному условию или нет.

 
*Псевдо код:*
```c++
int a[n];
for(int i = 0;i < n;i++)
cin >> a[i];
for(int i = 0;i < n;i++) for(int j = 0;j < n;j++) if(i!=j && a[i]+a[j] == z) return true;

return false;
```  
Ниже приведена реализация описанного подхода:  
```c++
#include <bits/stdc++.h>
using namespace std;
// Функция для нахождения пары в массиве
// чья сумма равна z
bool findPair(int a[], int n, int z)
{
// итерируем через все пары
for (int i = 0; i < n; i++)
for (int j = 0; j < n; j++)
// Проверка если сумма пары чисел
// (a[i], a[j]) равна z
if (i != j && a[i] + a[j] == z)
return true;
return false;
}
int main()
{
int a[] = { 1, -2, 1, 0, 5 };
int z = 0;
int n = sizeof(a) / sizeof(a[0]);
if (findPair(a, n, z))
cout << "True";
else
cout << "False";
return 0;
}
```
В предположении, что каждая из операций в компьютере занимает примерно
постоянное время, пусть это будет c. Количество выполняемых строк кода
фактически зависит от значения Z. При анализе алгоритма рассматривается в
основном наихудший вариант, т.е. когда не существует пары элементов, сумма
которых равна Z. В наихудшем случае для ввода требуется N*c операций.
Внешний цикл i выполняется N раз.
Для каждого i внутренний цикл j выполняется N раз.
Таким образом, общее время выполнения равно N*c + N*N*c + c. Теперь не будем
обращать внимания на члены низшего порядка, так как при больших входных
данных они относительно незначительны, поэтому берем только член высшего
порядка (без константы), который в данном случае равен N*N. Для описания
предельного поведения функции используются различные обозначения, но
поскольку берется наихудший случай, то для представления временной сложности
будет использоваться обозначение big-O.
Таким образом, для приведенного алгоритма временная сложность составляет
O(N2). Заметим, что временная сложность зависит исключительно от количества
элементов в массиве A, т.е. от длины входного сигнала, поэтому при увеличении
длины массива время выполнения алгоритма также будет увеличиваться.
Порядок роста - это зависимость времени выполнения от длины входных данных.
В приведенном примере хорошо видно, что время выполнения
квадратично зависит от длины массива. Порядок роста поможет легко вычислить
время выполнения.

#### Другой пример: Рассчитаем временную сложность приведенного ниже алгоритма

```c++
count = 0
for (int i = N; i > 0; i /= 2)
for (int j = 0; j < i; j++)
count++;
```
Это непростой случай. На первый взгляд кажется, что сложность составляет **O(N *
log N)**. **N** для цикла **j** и **log(N)** для цикла **i**. Но это не так. Давайте разберемся,
почему.
Подумайте, сколько раз будет выполняться **count++**.  
При **i = N** он выполнится **N** раз.  
При **i = N / 2** он выполнится **N / 2** раза.  
Когда **i = N / 4**, он выполнится **N / 4** раза.  
И т.д.  
Общее количество запусков **count++** равно **N + N/2 + N/4+...+1= 2 * N**. Таким
образом, временная сложность будет равна **O(N)**.
Ниже приведены некоторые общие временные сложности с указанием диапазона
входных данных, для которых они принимаются в конкурентном
программировании:
| **Input Length** | **Worst Accepted Time Complexity** |                   **Usually type of solutions**                  |
|:----------------:|:----------------------------------:|:----------------------------------------------------------------:|
|       10-12      | O(N!)                              | Recursion and backtracking                                       |
|       15-18      | O(2<sup>N</sup> * N)               |           Recursion, backtracking, and bit manipulation          |
|       18-22      | O(2<sup>N</sup> * N)               | Recursion, backtracking, and bit manipulation                    |
| 30-40            | O(2<sup>N/2 </sup> * N)            |              Meet in the middle, Divide and Conquer              |
| 100              | O(N<sup>4</sup>)                   | Dynamic programming, Constructive                                |
| 400              | O(N<sup>3</sup>)                   | Dynamic programming, Constructive                                |
| 2K               | O(N<sup>2</sup> * log N)           | Dynamic programming, Binary Search, Sorting,  Divide and Conquer |
| 10K              | O(N<sup>2</sup>)                   |          Dynamic programming, Graph, Trees, Constructive         |
| 1M               | O(N* log N)                        |            Sorting, Binary Search, Divide and Conquer            |
| 100M             |        O(N), O(log N), O(1)        |           Constructive, Mathematical, Greedy Algorithms          |



## Пространственная сложность
Решение задач с помощью компьютера требует памяти для хранения временных
данных или конечного результата в процессе выполнения программы. Объем
памяти, который требуется алгоритму для решения поставленной задачи,
называется пространственной сложностью алгоритма.
Пространственная сложность алгоритма - это количественная оценка объема
пространства, занимаемого алгоритмом для выполнения, как функция от длины
входных данных. Рассмотрим пример: Предположим, что требуется найти частоту
элементов массива.
Это объем памяти, необходимый для завершения работы алгоритма.
Чтобы оценить потребность в памяти, необходимо сосредоточиться на двух
составляющих:  
**Фиксированная часть** :
Она не зависит от размера входных данных. Она
включает в себя память для инструкций (кода), констант, переменных и т.д.
 
**Переменная часть** :
Зависит от размера входных данных. Она включает в себя
память для стека рекурсии, ссылающихся переменных и т.д.

#### Пример: сложение двух скалярных переменных

 > Алгоритм ADD SCALAR(A, B)         
 ```
    Описание: Выполнить арифметическое сложение двух чисел
    Вход: две скалярные переменные A и B
    Выход: переменная C, содержащая результат сложения A и B
    C <- A + B
    возврат C
 ```

Сложение двух скалярных чисел требует одной дополнительной ячейки памяти
для хранения результата. Таким образом, пространственная сложность данного
алгоритма постоянна, следовательно, **S(n) = O(1)**.  
  
Псевдокод выглядит следующим образом:
```c++
int freq[n];
int a[n];
for(int i = 0; i<n; i++)
{
cin>>a[i];
freq[a[i]]++;
}
```
Ниже приведена реализация описанного подхода:
```c++
#include <bits/stdc++.h>
using namespace std;
void countFreq(int arr[], int n) // Function to count frequencies of array items
{
unordered_map<int, int> freq;
// Traverse through array elements and
// count frequencies
for (int i = 0; i < n; i++) freq[arr[i]]++;
for (auto x : freq) cout << x.first << " " << x.second << endl; // Traverse through map and print frequencies
}
int main()
{
int arr[] = { 10, 20, 20, 10, 10, 20, 5, 20 };
int n = sizeof(arr) / sizeof(arr[0]);
countFreq(arr, n);
return 0;
}
```

***Output***
```
5 1
20 4
10 3
```
Здесь в алгоритме используются два массива длины **N** и переменная **i**, поэтому
общее занимаемое пространство равно **N * c + N * c + 1 * c = 2N * c + c**, где **c** -
единица занимаемого пространства. Для многих вводов константа **c**
несущественна, и можно сказать, что пространственная сложность равна **O(N)**.
Существует также вспомогательное пространство, которое отличается от
сложности пространства. Основное отличие заключается в том, что если
пространственная сложность определяет общее пространство, используемое
алгоритмом, то вспомогательное пространство определяет дополнительное
пространство, которое используется в алгоритме помимо заданного входного. В
приведенном выше примере вспомогательное пространство - это пространство,
используемое массивом **freq[]**, поскольку оно не является частью заданного
входного сигнала. Таким образом, общее вспомогательное пространство равно **N *
c + c**, что составляет всего **O(N)**.
